# ğŸ“‹ Resumen del Proyecto - Mango Supply Order Datathon 2025

## ğŸ¯ Objetivo

Predecir la cantidad Ã³ptima de producciÃ³n para cada producto de la nueva temporada de Mango, maximizando las ventas a precio completo (VAR) mientras se minimiza el exceso de stock y las ventas perdidas.

## ğŸ—ï¸ Estructura del Proyecto

```
supply_order_datathon25/
â”‚
â”œâ”€â”€ ğŸ“ data/                          # Datos del datathon
â”‚   â”œâ”€â”€ train.csv                     # Dataset de entrenamiento (colocar aquÃ­)
â”‚   â”œâ”€â”€ test.csv                      # Dataset de test (colocar aquÃ­)
â”‚   â”œâ”€â”€ sample_submission.csv         # Ejemplo de submission (colocar aquÃ­)
â”‚   â””â”€â”€ README.md                     # InformaciÃ³n sobre los datos
â”‚
â”œâ”€â”€ ğŸ“ models/                        # Modelos entrenados
â”‚   â”œâ”€â”€ xgboost_model.json           # Modelo XGBoost guardado
â”‚   â”œâ”€â”€ feature_importance.csv       # Importancia de features
â”‚   â””â”€â”€ model_metadata.json          # Metadatos del modelo
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                     # Notebooks de anÃ¡lisis
â”‚   â””â”€â”€ 01_exploratory_analysis.ipynb # AnÃ¡lisis exploratorio de datos
â”‚
â”œâ”€â”€ ğŸ“ src/                          # CÃ³digo fuente
â”‚   â”œâ”€â”€ __init__.py                  # InicializaciÃ³n del paquete
â”‚   â”œâ”€â”€ feature_engineering.py       # IngenierÃ­a de features
â”‚   â”œâ”€â”€ model.py                     # ImplementaciÃ³n del modelo XGBoost
â”‚   â””â”€â”€ utils.py                     # Funciones de utilidad
â”‚
â”œâ”€â”€ ğŸ“ submissions/                   # Archivos de submission
â”‚   â””â”€â”€ submission_YYYYMMDD_HHMMSS.csv
â”‚
â”œâ”€â”€ ğŸ“„ config.py                      # ConfiguraciÃ³n general
â”œâ”€â”€ ğŸ“„ config_custom_example.py       # Ejemplo de configuraciÃ³n personalizada
â”œâ”€â”€ ğŸ“„ train.py                       # Script principal de entrenamiento
â”œâ”€â”€ ğŸ“„ predict.py                     # Script de predicciÃ³n
â”œâ”€â”€ ğŸ“„ setup.sh                       # Script de configuraciÃ³n inicial
â”œâ”€â”€ ğŸ“„ requirements.txt               # Dependencias del proyecto
â”œâ”€â”€ ğŸ“„ README.md                      # DocumentaciÃ³n principal
â”œâ”€â”€ ğŸ“„ QUICK_START.md                 # GuÃ­a rÃ¡pida de inicio
â””â”€â”€ ğŸ“„ .gitignore                     # Archivos a ignorar en Git
```

## ğŸš€ Flujo de Trabajo

### 1. ConfiguraciÃ³n Inicial

```bash
bash setup.sh
# o manualmente:
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. ExploraciÃ³n de Datos (Opcional)

```bash
jupyter notebook notebooks/01_exploratory_analysis.ipynb
```

### 3. Entrenamiento del Modelo

```bash
# Entrenamiento bÃ¡sico (rÃ¡pido)
python train.py

# Con optimizaciÃ³n de hiperparÃ¡metros (mejor rendimiento)
python train.py --optimize
```

### 4. GeneraciÃ³n de Predicciones

Las predicciones se generan automÃ¡ticamente durante el entrenamiento.
Para solo generar predicciones con un modelo ya entrenado:

```bash
python predict.py
```

### 5. Submission

El archivo de submission se guarda en `submissions/submission_YYYYMMDD_HHMMSS.csv`
SÃºbelo a la plataforma del datathon.

## ğŸ”§ Componentes Principales

### Feature Engineering (`src/feature_engineering.py`)

Crea mÃ¡s de 50+ features derivadas:

- **Features Temporales**: mes, trimestre, temporada, etc.
- **Features Agregadas**: estadÃ­sticas por familia, categorÃ­a, temporada
- **Features de InteracciÃ³n**: capacidad total, potencial de ingresos, exposiciÃ³n
- **Features de Embeddings**: estadÃ­sticas de embeddings de imagen
- **Features de Lag**: producciÃ³n de temporadas anteriores

### Modelo (`src/model.py`)

ImplementaciÃ³n de XGBoost con:

- **ValidaciÃ³n Cruzada**: K-Fold con 5 splits
- **MÃ©tricas Personalizadas**: Score VAR, ventas perdidas, exceso de stock
- **OptimizaciÃ³n**: Optuna para bÃºsqueda de hiperparÃ¡metros
- **Early Stopping**: PrevenciÃ³n de overfitting
- **Feature Importance**: AnÃ¡lisis de importancia de features

### Utilidades (`src/utils.py`)

Funciones auxiliares para:

- Carga y guardado de datos
- CÃ¡lculo de estadÃ­sticas
- ValidaciÃ³n de submissions
- DetecciÃ³n de outliers
- ComparaciÃ³n de distribuciones

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

| MÃ©trica | DescripciÃ³n | Objetivo |
|---------|-------------|----------|
| **Custom Score** | Score 0-100 que penaliza ventas perdidas 2x mÃ¡s que exceso | Maximizar |
| **VAR** | Ventas a precio completo / producciÃ³n | Maximizar |
| **RMSE** | Root Mean Squared Error | Minimizar |
| **MAE** | Mean Absolute Error | Minimizar |
| **RÂ²** | Coeficiente de determinaciÃ³n | Maximizar |
| **Lost Sales** | Ventas perdidas promedio por producto | Minimizar |
| **Excess Stock** | Exceso de stock promedio por producto | Minimizar |

## ğŸ›ï¸ ConfiguraciÃ³n de ParÃ¡metros

### ParÃ¡metros de XGBoost (en `config.py`)

```python
XGBOOST_PARAMS = {
    'max_depth': 8,              # Profundidad del Ã¡rbol
    'learning_rate': 0.05,       # Tasa de aprendizaje
    'n_estimators': 1000,        # NÃºmero de Ã¡rboles
    'subsample': 0.8,            # ProporciÃ³n de muestras
    'colsample_bytree': 0.8,     # ProporciÃ³n de features
    # ... mÃ¡s parÃ¡metros
}
```

### PersonalizaciÃ³n

Para personalizar parÃ¡metros:

1. Copia `config_custom_example.py` a `config_custom.py`
2. Modifica los parÃ¡metros segÃºn tus necesidades
3. Importa desde `config_custom` en lugar de `config`

## ğŸ”¬ Mejoras Implementadas

### âœ… Completado

- [x] Feature engineering completo (temporal, agregado, interacciÃ³n, lag)
- [x] Modelo XGBoost con validaciÃ³n cruzada
- [x] MÃ©tricas personalizadas (VAR, lost sales, excess stock)
- [x] OptimizaciÃ³n de hiperparÃ¡metros con Optuna
- [x] Feature importance analysis
- [x] Early stopping y regularizaciÃ³n
- [x] Scripts de entrenamiento y predicciÃ³n
- [x] DocumentaciÃ³n completa
- [x] Notebook de anÃ¡lisis exploratorio

### ğŸ”„ Posibles Mejoras Futuras

- [ ] Ensemble con LightGBM y CatBoost
- [ ] Modelos especÃ­ficos por familia de producto
- [ ] Feature selection automÃ¡tico
- [ ] Transfer learning con embeddings
- [ ] CalibraciÃ³n de predicciones
- [ ] ValidaciÃ³n temporal estratificada
- [ ] AnÃ¡lisis de errores por segmento
- [ ] Dashboard interactivo de resultados

## ğŸ“ˆ Resultados Esperados

Con la configuraciÃ³n por defecto, deberÃ­as obtener:

- **Custom Score**: 70-85 (en validaciÃ³n cruzada)
- **VAR**: 0.75-0.90
- **RMSE**: Variable segÃºn escala de datos
- **RÂ²**: 0.6-0.8

Con optimizaciÃ³n de hiperparÃ¡metros:

- **Custom Score**: 80-90
- **VAR**: 0.80-0.95
- **RÂ²**: 0.7-0.85

## ğŸ› ï¸ Dependencias Principales

| Paquete | VersiÃ³n | PropÃ³sito |
|---------|---------|-----------|
| pandas | 2.1.4 | ManipulaciÃ³n de datos |
| numpy | 1.26.2 | Operaciones numÃ©ricas |
| xgboost | 2.0.3 | Modelo principal |
| scikit-learn | 1.3.2 | Utilidades de ML |
| optuna | 3.5.0 | OptimizaciÃ³n de hiperparÃ¡metros |
| matplotlib | 3.8.2 | VisualizaciÃ³n |
| seaborn | 0.13.0 | VisualizaciÃ³n estadÃ­stica |
| plotly | 5.18.0 | VisualizaciÃ³n interactiva |

## ğŸ’¡ Consejos y Mejores PrÃ¡cticas

### Para Mejor Performance

1. **Usa optimizaciÃ³n de hiperparÃ¡metros**: `python train.py --optimize`
2. **Analiza feature importance**: Identifica y enfÃ³cate en features importantes
3. **Experimenta con features**: Modifica `src/feature_engineering.py`
4. **Valida con datos temporales**: Asegura que el modelo generaliza bien

### Para Debugging

1. **Revisa notebooks**: Analiza datos en `01_exploratory_analysis.ipynb`
2. **Verifica distribuciones**: Compara train vs test
3. **Analiza errores**: Identifica patrones en predicciones incorrectas
4. **Valida submission**: Usa `validate_submission()` de `utils.py`

### Para ExperimentaciÃ³n RÃ¡pida

1. **Reduce datos**: Usa subset para iteraciÃ³n rÃ¡pida
2. **Reduce CV splits**: Usa 3 en lugar de 5 splits
3. **Reduce early_stopping_rounds**: Para convergencia mÃ¡s rÃ¡pida
4. **Deshabilita optimizaciÃ³n**: Usa parÃ¡metros por defecto

## ğŸ› SoluciÃ³n de Problemas Comunes

### Error: "train.csv not found"
**SoluciÃ³n**: Coloca los archivos CSV en la carpeta `data/`

### Error: "ModuleNotFoundError"
**SoluciÃ³n**: `pip install -r requirements.txt`

### Memoria insuficiente
**SoluciÃ³n**: 
- Reduce el dataset: `df = df.sample(frac=0.8)`
- Reduce `n_estimators` en config
- Usa `tree_method='hist'` en XGBoost

### Predicciones muy altas/bajas
**SoluciÃ³n**:
- Revisa feature engineering
- Ajusta parÃ¡metros de regularizaciÃ³n
- Usa `clip` en post-procesamiento

### Overfitting
**SoluciÃ³n**:
- Aumenta regularizaciÃ³n (`reg_alpha`, `reg_lambda`)
- Reduce `max_depth`
- Aumenta `min_child_weight`
- Reduce `learning_rate` y aumenta `n_estimators`

## ğŸ“š Referencias y Recursos

### DocumentaciÃ³n

- [XGBoost Docs](https://xgboost.readthedocs.io/)
- [Optuna Docs](https://optuna.readthedocs.io/)
- [Scikit-learn Docs](https://scikit-learn.org/)
- [Pandas Docs](https://pandas.pydata.org/)

### Papers y ArtÃ­culos

- Chen & Guestrin (2016): "XGBoost: A Scalable Tree Boosting System"
- Demand Forecasting in Fashion Retail
- Time Series Forecasting with Machine Learning

### Competencias Similares

- Kaggle: "Demand Forecasting"
- Kaggle: "Retail Sales Prediction"
- DrivenData: "Supply Chain Optimization"

## ğŸ‘¥ ContribuciÃ³n

Para contribuir al proyecto:

1. Fork el repositorio
2. Crea una rama: `git checkout -b feature/nueva-feature`
3. Commit cambios: `git commit -m 'Add nueva feature'`
4. Push: `git push origin feature/nueva-feature`
5. Abre un Pull Request

## ğŸ“„ Licencia

MIT License - Ver [LICENSE](LICENSE)

## ğŸ™ Agradecimientos

- **Mango** por organizar este desafÃ­o
- **Comunidad open-source** por las herramientas
- **Participantes** del datathon

---

**Â¡Ã‰xito en el datathon! ğŸ¥­ğŸš€**

*Para mÃ¡s informaciÃ³n, consulta [README.md](README.md) o [QUICK_START.md](QUICK_START.md)*

